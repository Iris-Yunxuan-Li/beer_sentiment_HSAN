{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Beer Review Classification with Hierarchical Self-Attention Networks\n",
        "\n",
        "## 1. Setup and Dependencies"
      ],
      "metadata": {
        "id": "N9Ei_MOnQAC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW-cpZznP6lb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from scipy.sparse import csr_matrix\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.utils.data import TensorDataset\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle\n",
        "import argparse\n",
        "from random import shuffle\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sys\n",
        "import datetime\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from operator import itemgetter\n",
        "from torch.autograd import Variable\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download NLTK tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "kwOI5uQSQQy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data directory\n",
        "datadir = \"/content/drive/MyDrive/ML CLASS/HW7/\"\n",
        "\n",
        "# Load the data\n",
        "with ZipFile(datadir+'labeled.json.zip', 'r') as ZZ:\n",
        "    for filename in ZZ.namelist():\n",
        "        with ZZ.open(filename, 'r') as f:\n",
        "            beer_reviews = json.load(f)\n",
        "\n",
        "# Dictionary to store overall ratings for each beer and brewer\n",
        "beer_ratings = {}\n",
        "brewer_ratings = {}\n",
        "\n",
        "# Extract overall ratings for each beer and brewer\n",
        "for review in beer_reviews:\n",
        "    beer_name = review['beer_name']\n",
        "    brewer_name = review['brewer']\n",
        "    overall_rating = review['overall']\n",
        "\n",
        "    # Update beer ratings\n",
        "    if beer_name not in beer_ratings:\n",
        "        beer_ratings[beer_name] = []\n",
        "    beer_ratings[beer_name].append(overall_rating)\n",
        "\n",
        "    # Update brewer ratings\n",
        "    if brewer_name not in brewer_ratings:\n",
        "        brewer_ratings[brewer_name] = []\n",
        "    brewer_ratings[brewer_name].append(overall_rating)\n",
        "\n",
        "# Calculate statistics for beers\n",
        "beer_stats = {}\n",
        "for beer_name, ratings in beer_ratings.items():\n",
        "    beer_stats[beer_name] = {\n",
        "        'mean': np.mean(ratings),\n",
        "        'median': np.median(ratings),\n",
        "        'std_dev': np.std(ratings)\n",
        "    }\n",
        "\n",
        "# Calculate statistics for brewers\n",
        "brewer_stats = {}\n",
        "for brewer_name, ratings in brewer_ratings.items():\n",
        "    brewer_stats[brewer_name] = {\n",
        "        'mean': np.mean(ratings),\n",
        "        'median': np.median(ratings),\n",
        "        'std_dev': np.std(ratings)\n",
        "    }\n",
        "\n",
        "# Print statistics\n",
        "print(\"Beer Statistics:\")\n",
        "for beer_name, stats in beer_stats.items():\n",
        "    print(f\"Beer: {beer_name}, Mean: {stats['mean']}, Median: {stats['median']}, Std Dev: {stats['std_dev']}\")\n",
        "\n",
        "print(\"\\nBrewer Statistics:\")\n",
        "for brewer_name, stats in brewer_stats.items():\n",
        "    print(f\"Brewer: {brewer_name}, Mean: {stats['mean']}, Median: {stats['median']}, Std Dev: {stats['std_dev']}\")"
      ],
      "metadata": {
        "id": "ZVBCkznEQTqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Vocabulary and Embeddings"
      ],
      "metadata": {
        "id": "__DKLUEgQZyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_vocab_emb():\n",
        "    vocab = 'word2idx_small'\n",
        "    with open(datadir+vocab+'.json', 'r') as f:\n",
        "        w2i = json.load(f)\n",
        "        num_words = len(w2i.keys())\n",
        "        print('NUM WORDS', num_words)\n",
        "\n",
        "    # Load pre-trained word embeddings\n",
        "    word2vec = {}\n",
        "    start = time.time()\n",
        "    with ZipFile(datadir+'glove.6B.50d.txt.zip', 'r') as ZZ:\n",
        "        for filename in ZZ.namelist():\n",
        "            with ZZ.open(filename, 'r') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    values = line.split()\n",
        "                    word = values[0]\n",
        "                    vec = np.asarray(values[1:], dtype='float32')\n",
        "                    word2vec[word] = vec\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    WordEmbeddings = np.zeros((num_words+1, 50))  # Index 0 will be zero\n",
        "    for word, i in w2i.items():\n",
        "        if word in word2vec:\n",
        "            WordEmbeddings[i] = word2vec[word]\n",
        "\n",
        "    return WordEmbeddings, w2i"
      ],
      "metadata": {
        "id": "YTGiSQyJQcdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Preprocessing Functions"
      ],
      "metadata": {
        "id": "MCIxa5UJQe9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(s):\n",
        "    return s.translate(str.maketrans('', '', string.punctuation+\"\\n\"))\n",
        "\n",
        "def ConvertSentence2Word(s):\n",
        "    return word_tokenize(remove_punctuation(s).lower())\n",
        "\n",
        "def ConvertSent2Idx(s):\n",
        "    s_temp = [w for w in ConvertSentence2Word(s) if w in w2i]\n",
        "    temp = [w2i[w] for w in s_temp]\n",
        "    return temp\n",
        "\n",
        "def ConvertDoc2List(doc):\n",
        "    temp_doc = sent_tokenize(doc)\n",
        "    temp = [ConvertSent2Idx(sentence) for sentence in temp_doc if len(ConvertSent2Idx(sentence)) >= 1]\n",
        "    return temp\n",
        "\n",
        "def ConvertList2Array(docs):\n",
        "    ms = len(docs)\n",
        "    mw = len(max(docs, key=len))\n",
        "    result = np.zeros((ms, mw))\n",
        "    for i, line in enumerate(docs):\n",
        "        for j, word in enumerate(line):\n",
        "            result[i, j] = word\n",
        "    return result\n",
        "\n",
        "def data_to_array(X_t, Y_t):\n",
        "    X_t_data = []\n",
        "    Y_t_data = []\n",
        "    p = len(w2i.keys())\n",
        "    for i in range(len(X_t)):\n",
        "        X_input = ConvertDoc2List(X_t[i])\n",
        "        if len(X_input) < 1:\n",
        "            continue\n",
        "        X_input = torch.LongTensor(ConvertList2Array(X_input))\n",
        "        Y_t_data.append(Y_t[i])\n",
        "        X_t_data.append(X_input.to(device))\n",
        "    Y_t_data = torch.tensor(np.array(Y_t_data).reshape((-1, 1))).type(torch.long).to(device)\n",
        "    return X_t_data, Y_t_data"
      ],
      "metadata": {
        "id": "SXahThokQemr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load and Split Data"
      ],
      "metadata": {
        "id": "BHCL81jBQlVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(num, corpus):\n",
        "    if corpus == 'beer':\n",
        "        with ZipFile(datadir+'labeled.json.zip', 'r') as ZZ:\n",
        "            for filename in ZZ.namelist():\n",
        "                with ZZ.open(filename, 'r') as f:\n",
        "                    brv = json.load(f)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        for i, b in enumerate(brv):\n",
        "            if i < num:\n",
        "                X.append(b['review'])\n",
        "                v = b['overall']\n",
        "                y = 0\n",
        "                if v >= 14:\n",
        "                    y = 1\n",
        "                Y.append(y)\n",
        "        del brv\n",
        "    else:\n",
        "        npz = np.load(datadir + 'yelp_review_small.npz', allow_pickle=True)\n",
        "        data = npz['arr_0']\n",
        "        X = data[:, 0]  # Text\n",
        "        Y = data[:, 1]  # Label\n",
        "        Y = Y - 1\n",
        "        del data\n",
        "    return X, Y\n",
        "\n",
        "def get_data(X, Y):\n",
        "    X, Y = data_to_array(X, Y)\n",
        "    ii = np.int64(np.arange(0, len(X), 1))\n",
        "    np.random.shuffle(ii)\n",
        "    XX = [X[i] for i in ii]\n",
        "    X = XX\n",
        "    Y = Y[ii]\n",
        "    num = len(X)\n",
        "    nntr = np.int32(.8 * num)\n",
        "    nnva = np.int32(.82 * num)\n",
        "    X_train_data = X[0:nntr]\n",
        "    y_train_data = Y[0:nntr]\n",
        "    X_val_data = X[nntr:nnva]\n",
        "    y_val_data = Y[nntr:nnva]\n",
        "    X_test_data = X[nnva:num]\n",
        "    y_test_data = Y[nnva:num]\n",
        "    return X_train_data, y_train_data, X_val_data, y_val_data, X_test_data, y_test_data"
      ],
      "metadata": {
        "id": "uzG89A8tQorp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define the Models"
      ],
      "metadata": {
        "id": "RzNT2_dDQqTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.k_linear(x)\n",
        "        q = self.q_linear(x)\n",
        "        v = self.v_linear(x)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        scores = torch.matmul(scores, v)\n",
        "        return scores\n",
        "\n",
        "class TargetAttention(Module):\n",
        "    def __init__(self, input_dim, dropout_rate):\n",
        "        super(TargetAttention, self).__init__()\n",
        "        self.target = nn.Parameter(torch.empty((1, input_dim)))\n",
        "        nn.init.kaiming_uniform_(self.target)\n",
        "        self.input_dim = input_dim\n",
        "        self.sq_input_dim = np.sqrt(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def target_att(self, t, k, v):\n",
        "        out = torch.matmul(t, k.permute(0, 2, 1)) / self.sq_input_dim\n",
        "        sf = torch.softmax(out, 2)\n",
        "        targ_att = torch.matmul(sf, v)\n",
        "        return targ_att\n",
        "\n",
        "    def forward(self, input):\n",
        "        target = self.target.expand(input.size(0), -1)\n",
        "        return self.target_att(target, input, input)\n",
        "\n",
        "class ConvolutionalCELL(nn.Module):\n",
        "    def __init__(self, d_model, filter_sizes, num_filters):\n",
        "        super(ConvolutionalCELL, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=d_model, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_outs = [F.relu(conv(x)) for conv in self.convs]\n",
        "        conv_outs = [F.max_pool1d(co, co.size(2)).squeeze(2) for co in conv_outs]\n",
        "        x = torch.cat(conv_outs, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class HierarchicalSelfAttentionNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, filter_sizes, num_filters):\n",
        "        super(HierarchicalSelfAttentionNetwork, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(WordEmbeddings)\n",
        "        self.convolutions = ConvolutionalCELL(embedding_dim, filter_sizes, num_filters)\n",
        "        self.attention = SelfAttention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.convolutions(x)\n",
        "        x = self.attention(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vZP3KNfRQs5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training and Evaluation"
      ],
      "metadata": {
        "id": "oKPJUzxdQvbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, lr=0.001):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i in range(len(X_train)):\n",
        "            inputs = X_train[i]\n",
        "            labels = y_train[i]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss/len(X_train):.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            evaluate_model(model, X_val, y_val)\n",
        "\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X_val)):\n",
        "            inputs = X_val[i]\n",
        "            labels = y_val[i]\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "def test_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X_test)):\n",
        "            inputs = X_test[i]\n",
        "            labels = y_test[i]\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Load and prepare data\n",
        "X, Y = load_data(num=10000, corpus='beer')\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_data(X, Y)\n",
        "\n",
        "# Initialize and train the model\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 100\n",
        "\n",
        "model = HierarchicalSelfAttentionNetwork(embedding_dim, hidden_dim, filter_sizes, num_filters).to(device)\n",
        "train_model(model, X_train, y_train, X_val, y_val, epochs=10, lr=0.001)\n",
        "test_model(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "werFUzPQQzCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Conclusion\n",
        "The Hierarchical Self-Attention Network (HSAN) project demonstrates the integration of self-attention mechanisms and convolutional layers for sentiment analysis of beer reviews. The model was trained and evaluated, showing promising results in classifying reviews into positive or negative sentiments."
      ],
      "metadata": {
        "id": "--53hq5qQ2zi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3A1Sa9uQ6xQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}