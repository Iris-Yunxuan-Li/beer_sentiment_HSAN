{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Beer Review Classification with Hierarchical Self-Attention Networks\n",
        "\n",
        "## 1. Setup and Dependencies"
      ],
      "metadata": {
        "id": "N9Ei_MOnQAC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EW-cpZznP6lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bcdba20-cb5b-40c9-cec7-9a7d342a28cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from scipy.sparse import csr_matrix\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.utils.data import TensorDataset\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle\n",
        "import argparse\n",
        "from random import shuffle\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import sys\n",
        "import datetime\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from operator import itemgetter\n",
        "from torch.autograd import Variable\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Inspection"
      ],
      "metadata": {
        "id": "kwOI5uQSQQy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datadir = \"/content/drive/MyDrive/ML CLASS/HW7/\"\n",
        "\n",
        "# Load the data\n",
        "with ZipFile(datadir+'labeled.json.zip', 'r') as ZZ:\n",
        "    for filename in ZZ.namelist():\n",
        "        with ZZ.open(filename, 'r') as f:\n",
        "            beer_reviews = json.load(f)\n",
        "\n",
        "# Dictionary to store overall ratings for each beer and brewer\n",
        "beer_ratings = {}\n",
        "brewer_ratings = {}\n",
        "\n",
        "# Extract overall ratings for each beer and brewer\n",
        "for review in beer_reviews:\n",
        "    beer_name = review['beer_name']\n",
        "    brewer_name = review['brewer']\n",
        "    overall_rating = review['overall']\n",
        "\n",
        "    # Update beer ratings\n",
        "    if beer_name not in beer_ratings:\n",
        "        beer_ratings[beer_name] = []\n",
        "    beer_ratings[beer_name].append(overall_rating)\n",
        "\n",
        "    # Update brewer ratings\n",
        "    if brewer_name not in brewer_ratings:\n",
        "        brewer_ratings[brewer_name] = []\n",
        "    brewer_ratings[brewer_name].append(overall_rating)\n",
        "\n",
        "# Statistics for beers\n",
        "beer_stats = {}\n",
        "for beer_name, ratings in beer_ratings.items():\n",
        "    beer_stats[beer_name] = {\n",
        "        'mean': np.mean(ratings),\n",
        "        'median': np.median(ratings),\n",
        "        'std_dev': np.std(ratings)\n",
        "    }\n",
        "\n",
        "# Statistics for brewers\n",
        "brewer_stats = {}\n",
        "for brewer_name, ratings in brewer_ratings.items():\n",
        "    brewer_stats[brewer_name] = {\n",
        "        'mean': np.mean(ratings),\n",
        "        'median': np.median(ratings),\n",
        "        'std_dev': np.std(ratings)\n",
        "    }\n",
        "\n",
        "# # Print\n",
        "# print(\"Beer Statistics:\")\n",
        "# for beer_name, stats in beer_stats.items():\n",
        "#     print(f\"Beer: {beer_name}, Mean: {stats['mean']}, Median: {stats['median']}, Std Dev: {stats['std_dev']}\")\n",
        "\n",
        "# print(\"\\nBrewer Statistics:\")\n",
        "# for brewer_name, stats in brewer_stats.items():\n",
        "#     print(f\"Brewer: {brewer_name}, Mean: {stats['mean']}, Median: {stats['median']}, Std Dev: {stats['std_dev']}\")"
      ],
      "metadata": {
        "id": "ZVBCkznEQTqq",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Vocabulary and Embeddings"
      ],
      "metadata": {
        "id": "__DKLUEgQZyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_vocab_emb():\n",
        "    vocab = 'word2idx_small'\n",
        "    with open(datadir+vocab+'.json', 'r') as f:\n",
        "        w2i = json.load(f)\n",
        "        num_words = len(w2i.keys())\n",
        "        print('NUM WORDS', num_words)\n",
        "\n",
        "    # Load pre-trained word embeddings\n",
        "    word2vec = {}\n",
        "    start = time.time()\n",
        "    with ZipFile(datadir+'glove.6B.50d.txt.zip', 'r') as ZZ:\n",
        "        for filename in ZZ.namelist():\n",
        "            with ZZ.open(filename, 'r') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    values = line.split()\n",
        "                    word = values[0]\n",
        "                    vec = np.asarray(values[1:], dtype='float32')\n",
        "                    word2vec[word] = vec\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    WordEmbeddings = np.zeros((num_words+1, 50))\n",
        "    for word, i in w2i.items():\n",
        "        if word in word2vec:\n",
        "            WordEmbeddings[i] = word2vec[word]\n",
        "\n",
        "    return WordEmbeddings, w2i"
      ],
      "metadata": {
        "id": "YTGiSQyJQcdW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Preprocessing Functions"
      ],
      "metadata": {
        "id": "MCIxa5UJQe9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(s):\n",
        "    return s.translate(str.maketrans('', '', string.punctuation+\"\\n\"))\n",
        "\n",
        "def ConvertSentence2Word(s):\n",
        "    return word_tokenize(remove_punctuation(s).lower())\n",
        "\n",
        "def ConvertSent2Idx(s):\n",
        "    s_temp = [w for w in ConvertSentence2Word(s) if w in w2i]\n",
        "    temp = [w2i[w] for w in s_temp]\n",
        "    return temp\n",
        "\n",
        "def ConvertDoc2List(doc):\n",
        "    temp_doc = sent_tokenize(doc)\n",
        "    temp = [ConvertSent2Idx(sentence) for sentence in temp_doc if len(ConvertSent2Idx(sentence)) >= 1]\n",
        "    return temp\n",
        "\n",
        "def ConvertList2Array(docs):\n",
        "    ms = len(docs)\n",
        "    mw = len(max(docs, key=len))\n",
        "    result = np.zeros((ms, mw))\n",
        "    for i, line in enumerate(docs):\n",
        "        for j, word in enumerate(line):\n",
        "            result[i, j] = word\n",
        "    return result\n",
        "\n",
        "def data_to_array(X_t, Y_t):\n",
        "    X_t_data = []\n",
        "    Y_t_data = []\n",
        "    p = len(w2i.keys())\n",
        "    for i in range(len(X_t)):\n",
        "        X_input = ConvertDoc2List(X_t[i])\n",
        "        if len(X_input) < 1:\n",
        "            continue\n",
        "        X_input = torch.LongTensor(ConvertList2Array(X_input))\n",
        "        Y_t_data.append(Y_t[i])\n",
        "        X_t_data.append(X_input.to(device))\n",
        "    Y_t_data = torch.tensor(np.array(Y_t_data).reshape((-1, 1))).type(torch.long).to(device)\n",
        "    return X_t_data, Y_t_data"
      ],
      "metadata": {
        "id": "SXahThokQemr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load and Split Data"
      ],
      "metadata": {
        "id": "BHCL81jBQlVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(num, corpus):\n",
        "    if corpus == 'beer':\n",
        "        with ZipFile(datadir+'labeled.json.zip', 'r') as ZZ:\n",
        "            for filename in ZZ.namelist():\n",
        "                with ZZ.open(filename, 'r') as f:\n",
        "                    brv = json.load(f)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "        for i, b in enumerate(brv):\n",
        "            if i < num:\n",
        "                X.append(b['review'])\n",
        "                v = b['overall']\n",
        "                y = 0\n",
        "                if v >= 14:\n",
        "                    y = 1\n",
        "                Y.append(y)\n",
        "        del brv\n",
        "    else:\n",
        "        npz = np.load(datadir + 'yelp_review_small.npz', allow_pickle=True)\n",
        "        data = npz['arr_0']\n",
        "        X = data[:, 0]  # Text\n",
        "        Y = data[:, 1]  # Label\n",
        "        Y = Y - 1\n",
        "        del data\n",
        "    return X, Y\n",
        "\n",
        "def get_data(X, Y):\n",
        "    X, Y = data_to_array(X, Y)\n",
        "    ii = np.int64(np.arange(0, len(X), 1))\n",
        "    np.random.shuffle(ii)\n",
        "    XX = [X[i] for i in ii]\n",
        "    X = XX\n",
        "    Y = Y[ii]\n",
        "    num = len(X)\n",
        "    nntr = np.int32(.8 * num)\n",
        "    nnva = np.int32(.82 * num)\n",
        "    X_train_data = X[0:nntr]\n",
        "    y_train_data = Y[0:nntr]\n",
        "    X_val_data = X[nntr:nnva]\n",
        "    y_val_data = Y[nntr:nnva]\n",
        "    X_test_data = X[nnva:num]\n",
        "    y_test_data = Y[nnva:num]\n",
        "    return X_train_data, y_train_data, X_val_data, y_val_data, X_test_data, y_test_data"
      ],
      "metadata": {
        "id": "uzG89A8tQorp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num=10000\n",
        "X,Y=load_data(num,'beer')\n",
        "len(X)\n",
        "\n",
        "WordEmbeddings, w2i=prep_vocab_emb()\n",
        "X_train_data,  y_train_data, X_val_data,  y_val_data, X_test_data, y_test_data = get_data(X,Y)"
      ],
      "metadata": {
        "id": "-rV9lTXn1_lN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c983bfc5-b7fe-4c5f-98df-1ab71da8e45b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM WORDS 12089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define the Models\n",
        "### Attension layer and target attension\n"
      ],
      "metadata": {
        "id": "RzNT2_dDQqTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.k_linear(x)\n",
        "        q = self.q_linear(x)\n",
        "        v = self.v_linear(x)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        scores = torch.matmul(scores, v)\n",
        "        return scores\n",
        "\n",
        "class TargetAttention(Module):\n",
        "    def __init__(self, input_dim,dropout_rate):\n",
        "        super(TargetAttention, self).__init__()\n",
        "        # define the transfer embedding vector T\\in R^{1*d}\n",
        "        self.target = nn.Parameter(torch.empty((1,input_dim)))\n",
        "        nn.init.kaiming_uniform_(self.target)\n",
        "        self.input_dim=input_dim\n",
        "        self.sq_input_dim = np.sqrt(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    #define the target attention process: softmax(T*V^T/sqrt(d))*V\n",
        "    def target_att(self, t, k, v):\n",
        "        # Dimensions: batch_size x 1 x number_of_tokens\n",
        "        out=torch.matmul(t, k.permute(0,2,1))/self.sq_input_dim\n",
        "        sf=torch.softmax(out,2)\n",
        "        # Dimensions: batch_size x 1 x embedd_dim\n",
        "        targ=torch.matmul(sf,v)\n",
        "        # Dimensions: batch_size x embedd_dim x 1\n",
        "        targ= targ.permute(0, 2, 1)\n",
        "        return targ\n",
        "\n",
        "    def forward(self, inputk, input):\n",
        "        batch_size = input.size(0)\n",
        "        # Make the target parameter have the same nunber of dimensions as the inputk, input.\n",
        "        output = self.target_att(self.target.expand(batch_size, 1, self.input_dim), inputk, input)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "vZP3KNfRQs5u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Cell"
      ],
      "metadata": {
        "id": "2JHF4WrU7K9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalCELL(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_dim, dropout_rate):\n",
        "        super(ConvolutionalCELL, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: [batch_size, sequence_length, input_dim]\n",
        "        x = input.permute(0, 2, 1)  # Reshape for convolution: [batch_size, input_dim, sequence_length]\n",
        "        conv_output = self.conv1d(x)\n",
        "        conv_output = F.relu(conv_output)\n",
        "        conv_output = self.dropout(conv_output)  # Apply dropout\n",
        "\n",
        "        # Reshape back to original shape: [batch_size, input_dim, sequence_length]\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "\n",
        "        output = F.layer_norm(conv_output + input, conv_output.size()[1:])\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "AU6AbUpd7OmT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self attension cell\n"
      ],
      "metadata": {
        "id": "L_Mg8CHW7yTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CELL(nn.Module):\n",
        "    def __init__(self, input_dim, kernel_dim, dropout_rate): #convc_cnt=1\n",
        "        super(CELL, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.cell = SelfAttention(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x=input\n",
        "        hidden= F.relu(self.dropout(self.cell(x))+x)\n",
        "        output = F.layer_norm(hidden, hidden.size()[1:])\n",
        "\n",
        "        return output\n",
        "\n",
        "class Hierarchical(nn.Module):\n",
        "    def __init__(self, num_emb, input_dim, dropout_rate, pretrained_weight, untrained_weight):\n",
        "        super(Hierarchical, self).__init__()\n",
        "        # Define the initialization of embedding matrix\n",
        "        # One is using the pretrained weight matrix, the other is initialized randomly.\n",
        "        self.id2vec_pretrained = nn.Embedding(num_emb, 50, padding_idx=1)\n",
        "        self.id2vec_pretrained.weight.data.copy_(pretrained_weight)\n",
        "        self.id2vec_pretrained.weight.requires_grad=False\n",
        "        self.id2vec_untrained = nn.Embedding(num_emb, 50, padding_idx=1)\n",
        "        self.id2vec_untrained.weight.data.copy_(untrained_weight)\n",
        "        self.id2vec_untrained.requires_grad = True\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.loss=nn.CrossEntropyLoss()\n",
        "\n",
        "    def accuracy(self,y_pred,y):\n",
        "        _, pred=y_pred.max(1)\n",
        "        acc=pred.eq(y)\n",
        "\n",
        "        return acc\n",
        "\n",
        "class HSAN(Hierarchical):\n",
        "    def __init__(self, input_dim, num_class, kernel_dim,\n",
        "                  dropout_rate, num_emb, pretrained_weight, untrained_weight, ltype=\"SA\"):\n",
        "        super(HSAN, self).__init__(num_emb, input_dim, dropout_rate, pretrained_weight, untrained_weight)\n",
        "\n",
        "        self.ltype=ltype\n",
        "        if ltype == \"SA\":\n",
        "            self.cell = CELL(input_dim, kernel_dim, dropout_rate)\n",
        "        elif ltype == \"CO\":\n",
        "            self.cell = ConvolutionalCELL(input_dim, kernel_dim, dropout_rate)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid ltype value. Allowed values are 'SA' and 'CO'.\")\n",
        "\n",
        "        self.taw = TargetAttention(input_dim, dropout_rate)\n",
        "        self.tas = TargetAttention(input_dim, dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.cls = nn.Linear(input_dim, num_class)\n",
        "        nn.init.xavier_normal_(self.cls.weight)\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        # Get embedding produce an array of number_of_sentences x number_of_words x embedd_dim.\n",
        "        input = self.id2vec_untrained(x)\n",
        "\n",
        "        # Word level self-attention\n",
        "        # Out Dim: number_of_sentences x number_of_words x embedd_dim\n",
        "        hiddenw = self.cell(input)\n",
        "\n",
        "        # Out Dim: number_of_sentences x embedd_dim x 1\n",
        "        hiddenw = self.taw(hiddenw, hiddenw)\n",
        "\n",
        "        # Out Dim: 1 x number_of_sentences x embedd_dim (batch_size = 1)\n",
        "        hiddenw = hiddenw.permute(2, 0, 1)\n",
        "\n",
        "        # Sentence level self-attention\n",
        "        # Out Dim: 1 x number_of _sentences x embedd_dim\n",
        "        hiddens = self.cell(hiddenw)\n",
        "\n",
        "        # Out Dim: 1 x embedd_dim  x 1\n",
        "        hiddens = self.tas(hiddens, hiddens)\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.cls(hiddens.squeeze(-1))\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x,  y):\n",
        "        logits = self.predict(x)\n",
        "        if logits.shape[1] == 1:\n",
        "            logits = torch.cat((-logits, logits), dim=1)\n",
        "        loss = self.loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        return loss, accuracy"
      ],
      "metadata": {
        "id": "K0ldOX8B72Yj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val_score(model, data, labels):\n",
        "    #Test the model accuracy for the test data set (or valid data set)\n",
        "    correct = 0.\n",
        "    #l = labels.shape[1]\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        val_pred = model.predict(data[i])\n",
        "\n",
        "        #torch.Tensor([np.argmax(y).squeeze()])\n",
        "        acc = model.accuracy(val_pred, labels[i])\n",
        "        correct = correct + acc # Compute the total correct number\n",
        "    val_acc = correct/len(data)\n",
        "\n",
        "    return val_acc"
      ],
      "metadata": {
        "id": "RUMUENM38pbP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training of Convolutional Cell and Self Attension Cell and Evaluation"
      ],
      "metadata": {
        "id": "oKPJUzxdQvbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epochs(model,X_train_data, y_train_data, optimizer,step=0):\n",
        "\n",
        "  bestscore = 0.\n",
        "  seed=14543\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  print(\"Traing data size:\",len(y_train_data))\n",
        "\n",
        "  n_epochs = 5 # When we remove the dropout layer and softmax layer, it converges fast.\n",
        "  model.train()\n",
        "  train_acc=[]\n",
        "  valid_acc=[]\n",
        "  # Then we run the model in each epochs.\n",
        "  num_train=len(X_train_data)\n",
        "  ii=list(range(num_train))\n",
        "  t1=time.time()\n",
        "  end=time.time()\n",
        "  for epoch in range(n_epochs):\n",
        "      start = time.time()\n",
        "      np.random.shuffle(ii)\n",
        "      accuracy = 0.\n",
        "      tot_loss=0.\n",
        "      for i,j in enumerate(ii):\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss, cor = model.forward(x=X_train_data[j],y=y_train_data[j])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          accuracy = accuracy + cor.item()\n",
        "          tot_loss+=loss.item()\n",
        "          if (np.mod(i,5000)==0 or i==num_train-1) and i>0:\n",
        "            val_sc=val_score(model, X_val_data, y_val_data)\n",
        "            t2=time.time()\n",
        "            print(\"iter %i, loss, %.3f, training accuracy: %.2f, validation accuracy: %.2f, time: %.3f\" % (\n",
        "              i, tot_loss/i,accuracy/i, val_sc,t2-t1))\n",
        "            t1=t2\n",
        "      accuracy = accuracy / num_train\n",
        "      train_acc.append(accuracy)\n",
        "      # test the model on the valid set\n",
        "      model = model.eval()\n",
        "      valscore = val_score(model,X_val_data,y_val_data)\n",
        "      model = model.train()\n",
        "      valid_acc.append(valscore)\n",
        "\n",
        "      # save the best model\n",
        "      if valscore >= bestscore:\n",
        "          bestscore = valscore\n",
        "          save_path = torch.save(model,datadir+\"models/beer_py_small_\"+model.ltype+\"_step\"+str(epoch)+\".pkl\")\n",
        "      temptime = datetime.timedelta(seconds=round(time.time() - start))\n",
        "      print(\"epoch %i, training accuracy: %.2f, validation accuracy: %.2f,\" % (\n",
        "          epoch + 1, accuracy * 100, valscore * 100), \"time: \", temptime)\n",
        "      accuracy = 0\n",
        "\n",
        "  totaltime = datetime.timedelta(seconds=round(time.time() - end))\n",
        "  testscore = val_score(model, X_test_data, y_test_data)\n",
        "  print(\"\\ntest accuracy: %.2f\" % (testscore*100),\"%\")\n",
        "  print(\"\\nTime:\", totaltime)\n",
        "  return model, train_acc, valid_acc"
      ],
      "metadata": {
        "id": "werFUzPQQzCD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(WordEmbeddings,ltype=\"SA\"):\n",
        "    # Set the parameters\n",
        "    input_dim = WordEmbeddings.shape[1] # = 50\n",
        "    kernel_dim = 3  #\n",
        "    num_words = WordEmbeddings.shape[0] # number of words in the embedding matrix\n",
        "    dropout_rate = 0.1\n",
        "    pretrained_weight = torch.Tensor(WordEmbeddings) # Word Embedding matrix 1\n",
        "    untrained_weight = torch.Tensor(WordEmbeddings.shape[0],WordEmbeddings.shape[1])\n",
        "    untrained_weight = nn.init.xavier_normal_(untrained_weight) # Word Embedding matrix 2\n",
        "    num_class = 2\n",
        "\n",
        "    # Define the model\n",
        "    model = HSAN(input_dim, num_class, kernel_dim,\n",
        "                dropout_rate, num_words, pretrained_weight, untrained_weight,ltype=ltype)\n",
        "    optimizer=torch.optim.Adam(model.parameters()) # define the optimizer function\n",
        "    num=0\n",
        "    for k,p in model.named_parameters():\n",
        "       if p.requires_grad:\n",
        "         print(k,p.shape)\n",
        "         num+=p.numel()\n",
        "    print('number of paramters',num)\n",
        "    # If we use gpu to run the model.\n",
        "    model = model.to(device)\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "q2W3-4IT84qY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Self-Attention Cell\n",
        "print(\"Running Self-Attention Cell\")\n",
        "self_att_model, optimizer = setup_model(WordEmbeddings, ltype=\"SA\")\n",
        "self_att_model, _, _ = run_epochs(self_att_model, X_train_data, y_train_data, optimizer)"
      ],
      "metadata": {
        "id": "t1tWf0tD85p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd2c4d1-decc-44aa-a893-44d25b648700"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Self-Attention Cell\n",
            "id2vec_untrained.weight torch.Size([12090, 50])\n",
            "cell.cell.q_linear.weight torch.Size([50, 50])\n",
            "cell.cell.q_linear.bias torch.Size([50])\n",
            "cell.cell.v_linear.weight torch.Size([50, 50])\n",
            "cell.cell.v_linear.bias torch.Size([50])\n",
            "cell.cell.k_linear.weight torch.Size([50, 50])\n",
            "cell.cell.k_linear.bias torch.Size([50])\n",
            "taw.target torch.Size([1, 50])\n",
            "tas.target torch.Size([1, 50])\n",
            "cls.weight torch.Size([2, 50])\n",
            "cls.bias torch.Size([2])\n",
            "number of paramters 612352\n",
            "Traing data size: 7972\n",
            "iter 5000, loss, 0.490, training accuracy: 0.78, validation accuracy: 0.81, time: 69.366\n",
            "iter 7971, loss, 0.471, training accuracy: 0.79, validation accuracy: 0.82, time: 43.540\n",
            "epoch 1, training accuracy: 79.04, validation accuracy: 81.91, time:  0:01:54\n",
            "iter 5000, loss, 0.368, training accuracy: 0.85, validation accuracy: 0.84, time: 74.238\n",
            "iter 7971, loss, 0.370, training accuracy: 0.85, validation accuracy: 0.82, time: 43.370\n",
            "epoch 2, training accuracy: 84.76, validation accuracy: 81.41, time:  0:01:57\n",
            "iter 5000, loss, 0.306, training accuracy: 0.87, validation accuracy: 0.80, time: 78.960\n",
            "iter 7971, loss, 0.315, training accuracy: 0.87, validation accuracy: 0.79, time: 44.982\n",
            "epoch 3, training accuracy: 86.55, validation accuracy: 80.90, time:  0:02:04\n",
            "iter 5000, loss, 0.258, training accuracy: 0.89, validation accuracy: 0.78, time: 73.536\n",
            "iter 7971, loss, 0.264, training accuracy: 0.89, validation accuracy: 0.78, time: 42.830\n",
            "epoch 4, training accuracy: 88.96, validation accuracy: 78.89, time:  0:01:57\n",
            "iter 5000, loss, 0.213, training accuracy: 0.91, validation accuracy: 0.79, time: 73.387\n",
            "iter 7971, loss, 0.225, training accuracy: 0.91, validation accuracy: 0.77, time: 43.341\n",
            "epoch 5, training accuracy: 90.54, validation accuracy: 77.89, time:  0:01:57\n",
            "\n",
            "test accuracy: 80.10 %\n",
            "\n",
            "Time: 0:09:48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Convolutional Cell\n",
        "print(\"\\nRunning Convolutional Cell\")\n",
        "conv_model, optimizer = setup_model(WordEmbeddings, ltype=\"CO\")\n",
        "conv_model, _, _ = run_epochs(conv_model, X_train_data, y_train_data, optimizer)"
      ],
      "metadata": {
        "id": "D3A1Sa9uQ6xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a93137b-2bb7-477f-e767-22d0970810c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Convolutional Cell\n",
            "id2vec_untrained.weight torch.Size([12090, 50])\n",
            "cell.conv1d.weight torch.Size([50, 50, 3])\n",
            "cell.conv1d.bias torch.Size([50])\n",
            "taw.target torch.Size([1, 50])\n",
            "tas.target torch.Size([1, 50])\n",
            "cls.weight torch.Size([2, 50])\n",
            "cls.bias torch.Size([2])\n",
            "number of paramters 612252\n",
            "Traing data size: 7972\n",
            "iter 5000, loss, 0.471, training accuracy: 0.79, validation accuracy: 0.81, time: 68.549\n",
            "iter 7971, loss, 0.455, training accuracy: 0.80, validation accuracy: 0.84, time: 41.634\n",
            "epoch 1, training accuracy: 79.59, validation accuracy: 84.42, time:  0:01:51\n",
            "iter 5000, loss, 0.338, training accuracy: 0.86, validation accuracy: 0.84, time: 74.308\n",
            "iter 7971, loss, 0.341, training accuracy: 0.86, validation accuracy: 0.80, time: 42.763\n",
            "epoch 2, training accuracy: 85.69, validation accuracy: 78.89, time:  0:01:56\n",
            "iter 5000, loss, 0.251, training accuracy: 0.90, validation accuracy: 0.81, time: 72.616\n",
            "iter 7971, loss, 0.266, training accuracy: 0.89, validation accuracy: 0.81, time: 43.099\n",
            "epoch 3, training accuracy: 88.79, validation accuracy: 80.90, time:  0:01:56\n",
            "iter 5000, loss, 0.194, training accuracy: 0.92, validation accuracy: 0.82, time: 70.579\n",
            "iter 7971, loss, 0.202, training accuracy: 0.92, validation accuracy: 0.79, time: 41.723\n",
            "epoch 4, training accuracy: 91.87, validation accuracy: 79.40, time:  0:01:52\n",
            "iter 5000, loss, 0.138, training accuracy: 0.94, validation accuracy: 0.82, time: 71.984\n",
            "iter 7971, loss, 0.148, training accuracy: 0.94, validation accuracy: 0.81, time: 42.720\n",
            "epoch 5, training accuracy: 94.10, validation accuracy: 80.40, time:  0:01:55\n",
            "\n",
            "test accuracy: 78.54 %\n",
            "\n",
            "Time: 0:09:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare, the convolutional cell runs slightly faster than the self attension cell. This makes sense as the self attention cell has relatively more parameters (612352) than the convolutional cell (612252).\n",
        "\n",
        "The final accuracy is slightly better for HSAN but both compareble (80% vs 78.5%)."
      ],
      "metadata": {
        "id": "IuynW8KPh5ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Self Attention with Pretrained embeddings"
      ],
      "metadata": {
        "id": "ZctFzlO9mKhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hierarchical_2(nn.Module):\n",
        "    def __init__(self, num_emb, input_dim, dropout_rate, pretrained_weight, untrained_weight):\n",
        "        super(Hierarchical_2, self).__init__()\n",
        "        # Define the initialization of embedding matrix\n",
        "        # One is using the pretrained weight matrix, the other is initialized randomly.\n",
        "        self.id2vec_pretrained = nn.Embedding(num_emb, 50, padding_idx=1)\n",
        "        self.id2vec_pretrained.weight.data.copy_(pretrained_weight)\n",
        "        self.id2vec_pretrained.weight.requires_grad=False\n",
        "\n",
        "        if untrained_weight is not None:  # Check if untrained_weight is not None\n",
        "            self.id2vec_untrained = nn.Embedding(num_emb, 50, padding_idx=1)\n",
        "            self.id2vec_untrained.weight.data.copy_(untrained_weight)\n",
        "            self.id2vec_untrained.requires_grad = True\n",
        "        else:\n",
        "            self.id2vec_untrained = None\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def accuracy(self,y_pred,y):\n",
        "        # compute the number of correct of the prediction.\n",
        "        _, pred = y_pred.max(1)\n",
        "        acc = pred.eq(y)\n",
        "\n",
        "        return acc\n",
        "\n",
        "\n",
        "class HSAN_2(Hierarchical_2):\n",
        "    def __init__(self, input_dim, num_class, kernel_dim,\n",
        "                  dropout_rate, num_emb, pretrained_weight, untrained_weight=None, ltype=\"SA\"):\n",
        "        super(HSAN_2, self).__init__(num_emb, input_dim, dropout_rate, pretrained_weight, untrained_weight)\n",
        "\n",
        "        self.ltype = ltype\n",
        "        if ltype == \"SA\":\n",
        "            self.cell = CELL(input_dim, kernel_dim, dropout_rate)\n",
        "        elif ltype == \"Conv\":  # Conditionally use ConvolutionalCELL when ltype is \"Conv\"\n",
        "            self.cell = ConvolutionalCELL(input_dim, kernel_dim, dropout_rate)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid ltype value. Allowed values are 'SA' and 'Conv'.\")\n",
        "\n",
        "        self.saw = self.cell\n",
        "        self.sas = self.cell\n",
        "\n",
        "        self.taw = TargetAttention(input_dim, dropout_rate)\n",
        "        self.tas = TargetAttention(input_dim, dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.cls = nn.Linear(input_dim, num_class)\n",
        "        nn.init.xavier_normal_(self.cls.weight)\n",
        "\n",
        "        # Initialize the untrained embedding layer if untrained_weight is not None\n",
        "        if untrained_weight is not None:\n",
        "            self.id2vec_untrained.weight.data.copy_(untrained_weight)\n",
        "            self.id2vec_untrained.requires_grad = True\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        # Get embedding if id2vec_untrained is not None\n",
        "        if self.id2vec_untrained is not None:\n",
        "            input = self.id2vec_untrained(x)\n",
        "        else:\n",
        "            input = self.id2vec_pretrained(x)  # Use pretrained embeddings if untrained_weight is None\n",
        "\n",
        "        hiddenw = self.saw(input)\n",
        "        hiddenw = self.taw(hiddenw, hiddenw)\n",
        "        hiddenw = hiddenw.permute(2, 0, 1)\n",
        "        hiddens = self.sas(hiddenw)\n",
        "        hiddens = self.tas(hiddens, hiddens)\n",
        "        logits = self.cls(hiddens.squeeze(-1))\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x,  y):\n",
        "        logits = self.predict(x)\n",
        "        if logits.shape[1] == 1:\n",
        "            logits = torch.cat((-logits, logits), dim=1)\n",
        "        loss = self.loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        return loss, accuracy"
      ],
      "metadata": {
        "id": "yEiaokxQh_1h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model_pretrained_embeddings(WordEmbeddings, ltype=\"SA\"):\n",
        "    input_dim = WordEmbeddings.shape[1]\n",
        "    kernel_dim = 3\n",
        "    num_words = WordEmbeddings.shape[0]\n",
        "    dropout_rate = 0.1\n",
        "    pretrained_weight = torch.Tensor(WordEmbeddings)  # Use pretrained embeddings\n",
        "\n",
        "    # For pretrained embeddings, we set untrained_weight to None\n",
        "    untrained_weight = None\n",
        "\n",
        "    num_class = 2\n",
        "\n",
        "    # Define the model with pretrained embeddings\n",
        "    model = HSAN_2(input_dim, num_class, kernel_dim,\n",
        "                 dropout_rate, num_words, pretrained_weight, untrained_weight, ltype=ltype)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Print the number of parameters being updated\n",
        "    num_updated_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Number of parameters being updated:\", num_updated_params)\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "ees4tFP5mhWo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model using only pretrained embeddings\n",
        "print(\"Running with pretrained embeddings\")\n",
        "pretrained_model, optimizer = setup_model_pretrained_embeddings(WordEmbeddings, ltype=\"SA\")\n",
        "pretrained_model, _, _ = run_epochs(pretrained_model, X_train_data, y_train_data, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRvPqP9gmz2X",
        "outputId": "eb5a2637-26d3-45d8-f690-0b73f93a16b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with pretrained embeddings\n",
            "Number of parameters being updated: 7852\n",
            "Traing data size: 7972\n",
            "iter 5000, loss, 0.608, training accuracy: 0.71, validation accuracy: 0.68, time: 25.824\n",
            "iter 7971, loss, 0.608, training accuracy: 0.71, validation accuracy: 0.68, time: 14.378\n",
            "epoch 1, training accuracy: 71.31, validation accuracy: 67.84, time:  0:00:41\n",
            "iter 5000, loss, 0.606, training accuracy: 0.71, validation accuracy: 0.68, time: 29.012\n",
            "iter 7971, loss, 0.602, training accuracy: 0.71, validation accuracy: 0.68, time: 19.227\n",
            "epoch 2, training accuracy: 71.48, validation accuracy: 67.84, time:  0:00:49\n",
            "iter 5000, loss, 0.608, training accuracy: 0.71, validation accuracy: 0.68, time: 30.828\n",
            "iter 7971, loss, 0.602, training accuracy: 0.71, validation accuracy: 0.68, time: 14.250\n",
            "epoch 3, training accuracy: 71.48, validation accuracy: 67.84, time:  0:00:45\n",
            "iter 5000, loss, 0.602, training accuracy: 0.71, validation accuracy: 0.68, time: 25.898\n",
            "iter 7971, loss, 0.602, training accuracy: 0.71, validation accuracy: 0.68, time: 15.456\n",
            "epoch 4, training accuracy: 71.48, validation accuracy: 67.84, time:  0:00:42\n",
            "iter 5000, loss, 0.604, training accuracy: 0.71, validation accuracy: 0.68, time: 25.966\n",
            "iter 7971, loss, 0.602, training accuracy: 0.71, validation accuracy: 0.68, time: 15.243\n",
            "epoch 5, training accuracy: 71.48, validation accuracy: 67.84, time:  0:00:41\n",
            "\n",
            "test accuracy: 70.23 %\n",
            "\n",
            "Time: 0:03:37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to the untrained SA, which has 612352 parameters, the pretrained SA has way less parameters (7852).  The pretrained weights version has a worse accuracy."
      ],
      "metadata": {
        "id": "m5T0VUjUnPKr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0UEQtVKnNN7"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}
